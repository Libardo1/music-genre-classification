{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from anytree import Node, RenderTree,NodeMixin\n",
    "from anytree.dotexport import RenderTreeGraph\n",
    "import os.path;\n",
    "import datetime;\n",
    "import time;\n",
    "import pandas;\n",
    "import numpy as np;\n",
    "import ast;\n",
    "import math;\n",
    "import sys;\n",
    "LOG_DIR=\"log\";\n",
    "LOG_IMAGE=LOG_DIR+\"/image\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCSVFile(file):\n",
    "    data=pandas.read_csv(file,\",\",header=0, na_values='?', skipinitialspace=True);\n",
    "    return data;\n",
    "    pass;\n",
    "def readTrainData(dataset):    \n",
    "    return dataset.ix[:,6:], dataset.ix[:,4:5].astype(int),dataset.ix[:,5:6];\n",
    "    pass;\n",
    "\n",
    "def readTestData(dataset):    \n",
    "    return dataset.ix[:,6:], dataset.ix[:,4:5].astype(int),dataset.ix[:,5:6];\n",
    "    pass;\n",
    "\n",
    "def getTimestamp():\n",
    "    ts = datetime.datetime.fromtimestamp(time.time()).strftime('%d-%m-%Y-%H:%M:%S')\n",
    "    return ts;\n",
    "\n",
    "def createDir(self,directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory);\n",
    "        pass;\n",
    "\n",
    "def dropColumns(dataframe,colList):\n",
    "    for c in colList:\n",
    "        dataframe.drop([c], axis = 1, inplace = True);\n",
    "    pass;\n",
    "\n",
    "def printPlanerTree(root):\n",
    "    print(\"---------[Tree]----------\");\n",
    "    for pre, fill, node in RenderTree(root): \n",
    "        print(\"%s%s\" % (pre, node.name));   \n",
    "    pass;\n",
    "\n",
    "def saveTreeAsPNG(root,filename=None):\n",
    "    if(filename==None):\n",
    "        filename=\"gener_\"+getTimestamp();\n",
    "    RenderTreeGraph(root).to_picture(LOG_IMAGE+\"/\"+filename+\".png\");\n",
    "    print(\"Imaged Saved\")\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode(NodeMixin): # Add Node feature\n",
    "    def __init__(self, value_dic,df, feature,theta,class_count,parent=None):\n",
    "        super(DTNode, self).__init__()\n",
    "        self.parent = parent;\n",
    "        self.val=value_dic;\n",
    "        self.dataframe = df;\n",
    "        self.feature=feature;\n",
    "        self.theta = theta;  \n",
    "        self.node_height=(0 if parent==None else parent.node_height+1);\n",
    "        self.class_count=class_count;\n",
    "        self.totalrecord=sum(class_count);\n",
    "        self.isLeafNode=False;\n",
    "        self.setNodeName();\n",
    "        pass;\n",
    "    \n",
    "    def setNodeName(self):\n",
    "        if(self.feature==None and self.theta==None):\n",
    "            op=self.val[\"op\"];\n",
    "            sign=( \">\" if op==1 else \"<\" );\n",
    "            self.name = \"[\"+sign+\" \"+str(self.parent.theta)+\"] Leaf \"+str(self.class_count);\n",
    "            self.isLeafNode=True;\n",
    "        elif(self.theta==None):\n",
    "            self.name = self.feature+\" [ROOT] \"+str(self.class_count);\n",
    "            self.isLeafNode=False;\n",
    "        else:\n",
    "            self.name = self.feature+\" [Theta=\"+str(self.theta)+\"] \"+str(self.class_count);\n",
    "            self.isLeafNode=False;\n",
    "        pass;\n",
    "    \n",
    "    def setData(self,feature,theta):\n",
    "        self.feature=feature;\n",
    "        self.theta = theta;\n",
    "        self.setNodeName();\n",
    "        pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: all continous data\n",
    "# tree: binary\n",
    "# feature repitation: allowed \n",
    "class DecisionTree():\n",
    "    \n",
    "    dataframe=None;\n",
    "    no_of_class=10;#number of features 0 to k-1\n",
    "    operator={\"less\":-1,\"equal\":0,\"greater\":1};\n",
    "    output_col=None;\n",
    "    features=None;\n",
    "    visited_feature=None;\n",
    "    repetition_allowed=True\n",
    "    minus_infinity=-9999;\n",
    "    detail_log_enabled=True;\n",
    "    logging_enabled=True;\n",
    "    min_record_count=2;\n",
    "    root_node=None;\n",
    "    max_depth=10;\n",
    "    #-----------------------------------------\n",
    "    \n",
    "    def __init__(self,df,output_col):\n",
    "        self.dataframe=df;\n",
    "        self.output_col=output_col;\n",
    "        self.features=list(self.dataframe.columns);\n",
    "        self.features.remove(self.output_col);\n",
    "        self.no_of_features=len(self.features);\n",
    "        self.visited_feature=[];\n",
    "        \n",
    "    #assuming all data is continous\n",
    "    def splitDataset(self,df,feature,value_dic):\n",
    "        val=value_dic[\"val\"];\n",
    "        op=value_dic[\"op\"];        \n",
    "        subsetdf=None;\n",
    "        if(op==self.operator[\"equal\"]):\n",
    "            print(\"Error: Equal not supported\");\n",
    "            subsetdf=None;# no categorical data: Assumption        \n",
    "        elif(op==self.operator[\"less\"]):\n",
    "            subsetdf= df.loc[(df[feature]<=val)];\n",
    "            \n",
    "        elif(op==self.operator[\"greater\"]):\n",
    "            subsetdf= df.loc[(df[feature]>val)];            \n",
    "        \n",
    "        return subsetdf;\n",
    "    \n",
    "    #entropy function\n",
    "    def getEntropy(self,pci):\n",
    "        ent=-1*pci*math.log(pci,2);\n",
    "        return ent;\n",
    "    \n",
    "    #impurity function\n",
    "    def getImpurity(self,pci):        \n",
    "        imp=self.getEntropy(pci);\n",
    "        return imp;\n",
    "    \n",
    "    #Pr(c=i)= (# of c=i)/total\n",
    "    def getPci(self,df,ci):\n",
    "        p=0.0;#probablity\n",
    "        y=df[self.output_col];\n",
    "        total=len(y);\n",
    "        no_of_ci=(y==ci).sum();\n",
    "        if(no_of_ci!=0 and total!=0):\n",
    "            p=float(no_of_ci)/total;\n",
    "        return p;\n",
    "        pass;\n",
    "    \n",
    "    def getClassCount(self,df):\n",
    "        y=df[self.output_col];\n",
    "        count=np.zeros(self.no_of_class);\n",
    "        for ci in range(self.no_of_class):\n",
    "            count[ci]=(y==ci).sum();\n",
    "        return count.astype(int);\n",
    "            \n",
    "    #return sum of impurity for all classes\n",
    "    def getNetImpurity(self,df):\n",
    "        e=0;\n",
    "        for i in range(self.no_of_class):\n",
    "            pci=self.getPci(df,i);       \n",
    "            if(pci!=0):\n",
    "                e+=self.getImpurity(pci);            \n",
    "        return e;\n",
    "        pass;\n",
    "    \n",
    "    #feature is continous\n",
    "    def getFeatureVal(self,df,feature):\n",
    "        mean=df[feature].mean();\n",
    "        values=[{\"val\":mean,\"op\":self.operator[\"less\"]},{\"val\":mean,\"op\":self.operator[\"greater\"]}];\n",
    "        return values,mean;\n",
    "        pass;\n",
    "    \n",
    "    #find gain for the given feature\n",
    "    def getGain(self,df,feature):\n",
    "        #H(S)\n",
    "        imp_S=self.getNetImpurity(df);\n",
    "        values,theta=self.getFeatureVal(df,feature);\n",
    "        net_Sf=0;\n",
    "        total_row=df[feature].count();        \n",
    "        for val_dic in values:\n",
    "            self.detaillog(\"------[GAIN: \"+feature+\"]------------\")  \n",
    "            self.detaillog(\"df record count:\",self.getDFRecordCount(df));\n",
    "            self.detaillog(\"val:\",val_dic);                        \n",
    "            Sv=self.splitDataset(df,feature,val_dic);                        \n",
    "            self.detaillog(\"df record count:\",self.getDFRecordCount(Sv));\n",
    "            len_Sv=Sv[feature].count();\n",
    "            self.detaillog(\"len:\",len_Sv);                        \n",
    "            ratio=float(len_Sv)/total_row;                        \n",
    "            self.detaillog(\"ratio:\",ratio);            \n",
    "            imp_Sv=self.getNetImpurity(Sv);\n",
    "            self.detaillog(\"imp_sv:\",imp_Sv);             \n",
    "            net_Sf+=(ratio*imp_Sv); \n",
    "            self.detaillog(\"net_sf:\",net_Sf)\n",
    "        if(self.detail_log_enabled):\n",
    "            print(\"imp_s:\",imp_S,\" net_sv:\",net_Sf,\"  diff:\",imp_S-net_Sf)\n",
    "        gain=float(imp_S-net_Sf);        \n",
    "        return gain;    \n",
    "        pass;\n",
    "    \n",
    "    #Finds the best feature among all feature\n",
    "    #select my maximum gain\n",
    "    def getBestFeature(self,df):\n",
    "        \n",
    "        gain_list=np.zeros(self.no_of_features);\n",
    "        for i in range(self.no_of_features):\n",
    "            f=self.features[i];\n",
    "            self.detaillog(\"---->\",f);\n",
    "            if(self.repetition_allowed or (self.repetition_allowed==False and f not in visited_features)):\n",
    "                g=self.getGain(df,f);               \n",
    "            else:\n",
    "                g=self.minus_infinity;\n",
    "            gain_list[i]=g;\n",
    "            self.log(\"Gain_\"+self.features[i]+\":\",g);\n",
    "            \n",
    "        index=gain_list.argmax();  \n",
    "        feature=self.features[index];        \n",
    "        return feature;\n",
    "        pass;\n",
    "\n",
    "    \n",
    "    def attachChildNodes(self,parent_node,df,feature,values):\n",
    "        for val in values:\n",
    "            subdf=self.splitDataset(df,feature,val);  \n",
    "            #if feature of the node is not decided i.e None then its a leave node.\n",
    "            newnode=DTNode(val,subdf,None,None,self.getClassCount(subdf),parent_node);        \n",
    "    \n",
    "    #This will generate the Tree\n",
    "    def generateTree(self,dtnode):     \n",
    "        self.log(\"node height:\",dtnode.node_height);\n",
    "        if(dtnode.node_height>self.max_depth):\n",
    "            return;#donot do anything        \n",
    "        if(dtnode.totalrecord>=self.min_record_count):\n",
    "            df=dtnode.dataframe;\n",
    "            \n",
    "            best_feature=self.getBestFeature(df);\n",
    "            self.detaillog(\"###Best Feature:\",best_feature);\n",
    "            values,theta=self.getFeatureVal(df,best_feature);\n",
    "            dtnode.setData(best_feature,theta);\n",
    "            self.attachChildNodes(dtnode,df,best_feature,values);\n",
    "            \n",
    "            for child in dtnode.children:                \n",
    "                self.generateTree(child);\n",
    "            \n",
    "        pass;\n",
    "    \n",
    "        pass;\n",
    "    def createDecisionTree(self):  \n",
    "        best_feature=self.getBestFeature(df);\n",
    "        self.detaillog(\"###Best Feature:\",best_feature);\n",
    "        values,theta=self.getFeatureVal(df,best_feature);\n",
    "        root_node=DTNode(None,self.dataframe,best_feature,theta,self.getClassCount(df));\n",
    "        self.attachChildNodes(root_node,df,best_feature,values);  \n",
    "        self.log(\"node height:\",root_node.node_height);\n",
    "        for child in root_node.children:                \n",
    "            self.generateTree(child);\n",
    "        self.root_node=root_node;\n",
    "        return root_node;    \n",
    "        pass;\n",
    "    \n",
    "    #predicits the value of the class\n",
    "    def predictProbilityPerClass(self,p_input):\n",
    "        node=self.root_node;\n",
    "        while(node.isLeafNode==False):\n",
    "            val=p_input[node.feature];\n",
    "            #binary tree.left branch < theta and right is >\n",
    "            node= ( node.children[0] if(val<=node.theta) else node.children[1] )\n",
    "        \n",
    "        self.detaillog(\"class\",node.class_count);\n",
    "        prob=np.array(node.class_count).astype(float)/node.totalrecord;\n",
    "        self.detaillog(\"probabiliy:\",prob);\n",
    "        return prob;\n",
    "        pass;\n",
    "    \n",
    "    def predictClass(self,p_input):\n",
    "            prob=self.predictProbilityPerClass(p_input);\n",
    "            y=prob.argmax();\n",
    "            return y;\n",
    "        \n",
    "    #return no. of record in data frame    \n",
    "    def getDFRecordCount(self,df):\n",
    "        return df.count(axis=0)[0];\n",
    "    \n",
    "    def predictForDF(self,df):\n",
    "        rcount=self.getDFRecordCount(df);\n",
    "        y_list=[];\n",
    "        for i in range(rcount):\n",
    "            r=df.iloc[i];\n",
    "            y=self.predictClass(r);\n",
    "            y_list.append(y);\n",
    "        return y_list;\n",
    "    \n",
    "    #find error in prediction\n",
    "    def findError(self,y_pred,y_act):\n",
    "        size=len(y_act);\n",
    "        misclassifedPoints = (y_pred != y_act).sum()  ;\n",
    "        accuracy = (float(size - misclassifedPoints)*100) / size;\n",
    "        return misclassifedPoints,accuracy;\n",
    "        pass;\n",
    "    \n",
    "    def log(self,text,data=None):\n",
    "        if self.logging_enabled:\n",
    "            if(data!=None):\n",
    "                print(text,data);\n",
    "            else:\n",
    "                print(text);\n",
    "    def detaillog(self,text,data=None):\n",
    "        if self.detail_log_enabled:\n",
    "            if(data!=None):\n",
    "                print(text,data);\n",
    "            else:\n",
    "                print(text);\n",
    "        pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A    B    C    D\n",
      "0  1    2   30    4\n",
      "1  2    6   70    8\n",
      "2  2  208  101   12\n",
      "3  3  198  150  160\n",
      "-------------------\n",
      "('Gain_B:', 0.5)\n",
      "('Gain_C:', 0.5)\n",
      "('Gain_D:', 0.8112781244591328)\n",
      "('node height:', 0)\n",
      "('node height:', 1)\n",
      "('Gain_B:', 0.2516291673878229)\n",
      "('Gain_C:', 0.9182958340544896)\n",
      "('Gain_D:', 0.2516291673878229)\n",
      "('node height:', 2)\n",
      "('node height:', 2)\n",
      "('node height:', 1)\n",
      "---------[Tree]----------\n",
      "D [Theta=46.0] [0 1 2 1 0 0 0 0 0 0]\n",
      "├── C [Theta=67.0] [0 1 2 0 0 0 0 0 0 0]\n",
      "│   ├── [< 67.0] Leaf [0 1 0 0 0 0 0 0 0 0]\n",
      "│   └── [> 67.0] Leaf [0 0 2 0 0 0 0 0 0 0]\n",
      "└── [> 46.0] Leaf [0 0 0 1 0 0 0 0 0 0]\n",
      "Imaged Saved\n",
      "('y:', [1, 2, 2, 3])\n",
      "('misclassifed:', 0, ' accuracy:', 100.0)\n"
     ]
    }
   ],
   "source": [
    "#TEST DATA\n",
    "arr=np.array([[1,2,30,4],[2,6,70,8],[2,208,101,12],[3,198,150,160]])\n",
    "df = pandas.DataFrame(arr, columns=['A', 'B', 'C', 'D'])\n",
    "print(df)\n",
    "print(\"-------------------\");\n",
    "dt=DecisionTree(df,'A');\n",
    "dt.min_record_count=2;\n",
    "dt.max_depth=1;\n",
    "dt.detail_log_enabled=False;\n",
    "root=dt.createDecisionTree();\n",
    "printPlanerTree(root);\n",
    "saveTreeAsPNG(root);\n",
    "\n",
    "y_pred=dt.predictForDF(df)\n",
    "print(\"y:\",y_pred);\n",
    "m,a=dt.findError(y_pred,np.array(df['A']))\n",
    "print(\"misclassifed:\",m,\" accuracy:\",a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train', (801, 20), 'test', (200, 20))\n"
     ]
    }
   ],
   "source": [
    "# Music GENER CLASSIFICATION.....\n",
    "dir=\"data/\"\n",
    "trainFile=dir+\"train.csv\";\n",
    "testFile=dir+\"test.csv\";\n",
    "trained_dataset=readCSVFile(trainFile);\n",
    "test_dataset=readCSVFile(testFile);\n",
    "trained_data,trained_y,trained_y_vector=readTrainData(trained_dataset);\n",
    "test_data,test_y,test_y_vector=readTestData(test_dataset);\n",
    "\n",
    "mtx_train =trained_data.as_matrix(columns=None)\n",
    "mtx_train_y  =trained_y.as_matrix(columns=None)\n",
    "mtx_train_y=np.array(list((e[0] for e in mtx_train_y)));\n",
    "\n",
    "mtx_test=test_data.as_matrix(columns=None);\n",
    "mtx_test_y=test_y.as_matrix(columns=None);\n",
    "mtx_test_y=np.array(list((e[0] for e in mtx_test_y)));\n",
    "print(\"train\",np.shape(mtx_train),\"test\",np.shape(mtx_test));\n",
    "#Note: mtx_*** no in use\n",
    "#----------------------------------------------||||\n",
    "colList=[\"Unnamed: 0\",\"Unnamed: 0.1\",\"id\",\"type\",\"y\"];\n",
    "dropColumns(trained_dataset,colList);\n",
    "dropColumns(test_dataset,colList);\n",
    "\n",
    "#Note: Data frame in use 'trained_dataset' and 'test_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Started\n",
      "centroid var [Theta=462431.529081] [85 77 76 78 77 84 82 80 76 86]\n",
      "├── mfcc1 mean [Theta=-155.540549804] [76 77 55 40 24 75 80  6 20 61]\n",
      "│   ├── rms mean [Theta=-8.75202680887] [33 73 22  2  4 59  1  0 10  7]\n",
      "│   │   ├── rolloff var [Theta=870994.932992] [ 5 66  1  0  0 14  0  0  0  1]\n",
      "│   │   │   ├── rolloff var [Theta=408493.482407] [ 0 56  0  0  0  3  0  0  0  0]\n",
      "│   │   │   │   ├── centroid mean [Theta=1239.85527795] [ 0 35  0  0  0  0  0  0  0  0]\n",
      "│   │   │   │   │   ├── [< 1239.85527795] Leaf [ 0 17  0  0  0  0  0  0  0  0]\n",
      "│   │   │   │   │   └── [> 1239.85527795] Leaf [ 0 18  0  0  0  0  0  0  0  0]\n",
      "│   │   │   │   └── [> 408493.482407] Leaf [ 0 21  0  0  0  3  0  0  0  0]\n",
      "│   │   │   └── [> 870994.932992] Leaf [ 5 10  1  0  0 11  0  0  0  1]\n",
      "│   │   └── contrast mean [Theta=21.8843515067] [28  7 21  2  4 45  1  0 10  6]\n",
      "│   │       ├── mfcc5 var [Theta=154.074242234] [18  6  1  2  2 33  1  0  1  1]\n",
      "│   │       │   ├── mfcc4 mean [Theta=36.6581693688] [ 3  6  0  2  0 28  1  0  0  0]\n",
      "│   │       │   │   ├── [< 36.6581693688] Leaf [ 0  6  0  1  0 11  0  0  0  0]\n",
      "│   │       │   │   └── [> 36.6581693688] Leaf [ 3  0  0  1  0 17  1  0  0  0]\n",
      "│   │       │   └── [> 154.074242234] Leaf [15  0  1  0  2  5  0  0  1  1]\n",
      "│   │       └── contrast mean [Theta=25.9224438118] [10  1 20  0  2 12  0  0  9  5]\n",
      "│   │           ├── mfcc5 var [Theta=228.290933606] [ 7  0 10  0  2 12  0  0  2  0]\n",
      "│   │           │   ├── [< 228.290933606] Leaf [ 4  0  4  0  0 11  0  0  0  0]\n",
      "│   │           │   └── [> 228.290933606] Leaf [3 0 6 0 2 1 0 0 2 0]\n",
      "│   │           └── [> 25.9224438118] Leaf [ 3  1 10  0  0  0  0  0  7  5]\n",
      "│   └── mfcc4 mean [Theta=44.9332644518] [43  4 33 38 20 16 79  6 10 54]\n",
      "│       ├── mfcc5 mean [Theta=-3.58190304146] [ 9  4 20 30 11 14  4  6  7 29]\n",
      "│       │   ├── centroid mean [Theta=2223.37757926] [ 9  2  8 20  6  0  3  0  4 11]\n",
      "│       │   │   ├── mfcc4 mean [Theta=37.0828430693] [7 2 7 4 2 0 0 0 4 8]\n",
      "│       │   │   │   ├── [< 37.0828430693] Leaf [0 2 4 2 0 0 0 0 1 2]\n",
      "│       │   │   │   └── [> 37.0828430693] Leaf [7 0 3 2 2 0 0 0 3 6]\n",
      "│       │   │   └── [> 2223.37757926] Leaf [ 2  0  1 16  4  0  3  0  0  3]\n",
      "│       │   └── mfcc4 var [Theta=166.838607883] [ 0  2 12 10  5 14  1  6  3 18]\n",
      "│       │       ├── mfcc5 mean [Theta=6.29988684391] [ 0  2  9  2  3 13  0  2  0  8]\n",
      "│       │       │   ├── [< 6.29988684391] Leaf [ 0  2  0  2  3 10  0  0  0  3]\n",
      "│       │       │   └── [> 6.29988684391] Leaf [0 0 9 0 0 3 0 2 0 5]\n",
      "│       │       └── mfcc5 var [Theta=164.447858786] [ 0  0  3  8  2  1  1  4  3 10]\n",
      "│       │           ├── [< 164.447858786] Leaf [0 0 0 7 0 1 1 2 0 9]\n",
      "│       │           └── [> 164.447858786] Leaf [0 0 3 1 2 0 0 2 3 1]\n",
      "│       └── zero mean [Theta=0.125566017435] [34  0 13  8  9  2 75  0  3 25]\n",
      "│           ├── mfcc5 var [Theta=140.074989856] [24  0 13  6  6  2 17  0  2 15]\n",
      "│           │   ├── rolloff var [Theta=966660.563052] [11  0  4  6  0  2 13  0  1 13]\n",
      "│           │   │   ├── [< 966660.563052] Leaf [11  0  2  0  0  2  8  0  0  6]\n",
      "│           │   │   └── [> 966660.563052] Leaf [0 0 2 6 0 0 5 0 1 7]\n",
      "│           │   └── mfcc2 mean [Theta=103.965439351] [13  0  9  0  6  0  4  0  1  2]\n",
      "│           │       ├── [< 103.965439351] Leaf [5 0 2 0 6 0 3 0 0 2]\n",
      "│           │       └── [> 103.965439351] Leaf [8 0 7 0 0 0 1 0 1 0]\n",
      "│           └── centroid mean [Theta=2648.83370547] [10  0  0  2  3  0 58  0  1 10]\n",
      "│               ├── rms mean [Theta=2.83146731416] [ 8  0  0  2  2  0 21  0  1 10]\n",
      "│               │   ├── [< 2.83146731416] Leaf [ 4  0  0  2  1  0  8  0  0 10]\n",
      "│               │   └── [> 2.83146731416] Leaf [ 4  0  0  0  1  0 13  0  1  0]\n",
      "│               └── mfcc3 var [Theta=253.067669313] [ 2  0  0  0  1  0 37  0  0  0]\n",
      "│                   ├── [< 253.067669313] Leaf [ 0  0  0  0  0  0 27  0  0  0]\n",
      "│                   └── [> 253.067669313] Leaf [ 2  0  0  0  1  0 10  0  0  0]\n",
      "└── mfcc1 mean [Theta=-122.562378833] [ 9  0 21 38 53  9  2 74 56 25]\n",
      "    ├── mfcc5 var [Theta=246.692512834] [ 9  0 15  8 15  9  1  9 41 19]\n",
      "    │   ├── mfcc5 var [Theta=171.84178361] [ 4  0 10  7  6  9  1  6 13 16]\n",
      "    │   │   ├── contrast var [Theta=22.0189590525] [ 0  0  2  1  2  8  1  3  3 11]\n",
      "    │   │   │   ├── [< 22.0189590525] Leaf [0 0 0 0 0 7 0 0 1 9]\n",
      "    │   │   │   └── [> 22.0189590525] Leaf [0 0 2 1 2 1 1 3 2 2]\n",
      "    │   │   └── contrast mean [Theta=23.1779234719] [ 4  0  8  6  4  1  0  3 10  5]\n",
      "    │   │       ├── [< 23.1779234719] Leaf [1 0 3 5 0 1 0 3 6 5]\n",
      "    │   │       └── [> 23.1779234719] Leaf [3 0 5 1 4 0 0 0 4 0]\n",
      "    │   └── centroid mean [Theta=1939.7926698] [ 5  0  5  1  9  0  0  3 28  3]\n",
      "    │       ├── rms mean [Theta=-2.53688270545] [ 5  0  5  0  2  0  0  0 20  2]\n",
      "    │       │   ├── [< -2.53688270545] Leaf [4 0 3 0 0 0 0 0 2 2]\n",
      "    │       │   └── [> -2.53688270545] Leaf [ 1  0  2  0  2  0  0  0 18  0]\n",
      "    │       └── [> 1939.7926698] Leaf [0 0 0 1 7 0 0 3 8 1]\n",
      "    └── mfcc5 var [Theta=204.49926135] [ 0  0  6 30 38  0  1 65 15  6]\n",
      "        ├── mfcc1 mean [Theta=-62.9309674408] [ 0  0  5 25  6  0  1 44  4  5]\n",
      "        │   ├── mfcc4 var [Theta=240.591594203] [ 0  0  4 12  6  0  1  9  4  3]\n",
      "        │   │   ├── [< 240.591594203] Leaf [0 0 4 8 0 0 0 5 1 3]\n",
      "        │   │   └── [> 240.591594203] Leaf [0 0 0 4 6 0 1 4 3 0]\n",
      "        │   └── mfcc3 mean [Theta=8.66023984352] [ 0  0  1 13  0  0  0 35  0  2]\n",
      "        │       ├── [< 8.66023984352] Leaf [ 0  0  1 11  0  0  0 10  0  1]\n",
      "        │       └── [> 8.66023984352] Leaf [ 0  0  0  2  0  0  0 25  0  1]\n",
      "        └── mfcc4 mean [Theta=27.956339198] [ 0  0  1  5 32  0  0 21 11  1]\n",
      "            ├── mfcc3 mean [Theta=10.7145356074] [ 0  0  1  2  9  0  0 20  6  0]\n",
      "            │   ├── [< 10.7145356074] Leaf [0 0 1 2 8 0 0 4 4 0]\n",
      "            │   └── [> 10.7145356074] Leaf [ 0  0  0  0  1  0  0 16  2  0]\n",
      "            └── contrast var [Theta=29.2940288538] [ 0  0  0  3 23  0  0  1  5  1]\n",
      "                ├── [< 29.2940288538] Leaf [ 0  0  0  3 10  0  0  0  5  1]\n",
      "                └── [> 29.2940288538] Leaf [ 0  0  0  0 13  0  0  1  0  0]\n",
      "Imaged Saved\n",
      "('Train Data:', 'misclassifed:', 364, ' accuracy:', 54.55680399500624)\n",
      "('Test Data:', 'misclassifed:', 111, ' accuracy:', 44.5)\n"
     ]
    }
   ],
   "source": [
    "df=trained_dataset;\n",
    "dt=DecisionTree(df,'y_index');\n",
    "dt.min_record_count=20;\n",
    "dt.max_depth=10;\n",
    "dt.detail_log_enabled=False; # Print status\n",
    "dt.logging_enabled=False;# Print status\n",
    "print(\"training Started\");\n",
    "root=dt.createDecisionTree();\n",
    "printPlanerTree(root);\n",
    "saveTreeAsPNG(root);\n",
    "\n",
    "y_pred=dt.predictForDF(df)\n",
    "#print(\"y:\",y_pred);\n",
    "m,a=dt.findError(y_pred,np.array(df['y_index']))\n",
    "print(\"Train Data:\",\"misclassifed:\",m,\" accuracy:\",a);\n",
    "\n",
    "df=test_dataset\n",
    "y_pred=dt.predictForDF(df)\n",
    "#print(\"y:\",y_pred);\n",
    "m,a=dt.findError(y_pred,np.array(df['y_index']))\n",
    "print(\"Test Data:\",\"misclassifed:\",m,\" accuracy:\",a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
